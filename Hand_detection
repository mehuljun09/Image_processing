import cv2 
import mediapipe as mp 

from google.protobuf.json_format import MessageToDict 


hands = mp.solutions.hands.Hands( 
    static_image_mode=False, 
    model_complexity=1, 
    min_detection_confidence=0.75, 
    min_tracking_confidence=0.75, 
    max_num_hands=2) 

cap = cv2.VideoCapture(0) 

while 1: 
    success, img = cap.read() 
    # as camera reads a mirror image 
    img = cv2.flip(img, 1) 
    #openCV reads in BGR format
    imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) 
    results = hands.process(imgRGB) 

    if results.multi_hand_landmarks: 
            for i in results.multi_handedness:                 
                hand_dict = MessageToDict(i)
                label = hand_dict['classification'][0]['label'] 

                if label == 'Left': 
                    cv2.putText(img, label , 
                                (20, 60), 
                                cv2.FONT_HERSHEY_PLAIN, 
                                1.2, (0, 255, 0), 3) 
                #current frame,text displayed, (x,y),font,scaling,color,pixel thickness
                elif label == 'Right': 
                    cv2.putText(img, label, 
                                (300, 60), 
                                cv2.FONT_HERSHEY_PLAIN, 
                                1.2, (255, 0, 0), 3) 
    # display the current frame in window
    cv2.imshow('Image', img) 
    if cv2.waitKey(1) & 0xFF == ord('q'): 
        break

cap.release()
cv2.destroyAllWindows()
